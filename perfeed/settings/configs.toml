[config]
openai_model="gpt-4o-mini"
ollama_model="llama3.1:8b"
strict_load_by_model_provider=true # only load and return the data from store if generated by the same model and provider

[ollama]
auto_num_ctx = false # set to True if you don't want to manually set `num_ctx`
num_ctx = 32000 # the size of the context window used to generate the next token. See https://github.com/ollama/ollama/blob/main/docs/modelfile.md#instructions
num_ctx_buffer = 1.1 # number of contexts to buffer to prevent OOM
temperature = 0 # See https://github.com/ollama/ollama/blob/main/docs/modelfile.md#instructions