{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perfeed.tools.pr_summarizer import PRSummarizer\n",
    "from perfeed.git_providers.github import GithubProvider\n",
    "from perfeed.llms.ollama_client import OllamaClient\n",
    "from perfeed.llms.openai_client import OpenAIClient\n",
    "from perfeed.data_stores import FeatherStorage\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PR summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{    \"type\": [\"Enhancement\"],    \"title\": \"Add BaseClient for llm interface with OpenAI and Ollama impl\",    \"description\": \"This PR introduces a new `BaseClient` interface for LLM clients, along with implementations for `OpenAIClient` and `OllamaClient`. This structure allows for more flexible integration of various LLM clients. Additionally, it updates the `pyproject.toml` and `poetry.lock` files to include the new `ollama` package.\",    \"pr_files\": [        {            \"filename\": \"poetry.lock\",            \"language\": \"plaintext\",            \"changes_summary\": \"Added `ollama` package and updated dependencies for `openai` and `scikit_learn`.\",            \"changes_title\": \"Update dependencies and add ollama package\",            \"label\": \"dependencies\"        },        {            \"filename\": \"pyproject.toml\",            \"language\": \"plaintext\",            \"changes_summary\": \"Included `ollama` package in the project dependencies.\",            \"changes_title\": \"Add ollama package to project dependencies\",            \"label\": \"dependencies\"        },        {            \"filename\": \"src/llms/base_client.py\",            \"language\": \"python\",            \"changes_summary\": \"Created `BaseClient` abstract class defining the interface for LLM clients.\",            \"changes_title\": \"Define BaseClient interface for LLM clients\",            \"label\": \"enhancement\"        },        {            \"filename\": \"src/llms/ollama_client.py\",            \"language\": \"python\",            \"changes_summary\": \"Implemented `OllamaClient` class that extends `BaseClient` for interacting with the Ollama API.\",            \"changes_title\": \"Implement OllamaClient for Ollama API\",            \"label\": \"enhancement\"        },        {            \"filename\": \"src/llms/openai_client.py\",            \"language\": \"python\",            \"changes_summary\": \"Implemented `OpenAIClient` class that extends `BaseClient` for interacting with the OpenAI API.\",            \"changes_title\": \"Implement OpenAIClient for OpenAI API\",            \"label\": \"enhancement\"        }    ],    \"comments\": [        {            \"parent_thread_id\": 1803582344,            \"child_thread_ids\": [1803990837, 1804912347],            \"html_url\": \"https://github.com/Perfeed/perfeed/pull/8#discussion_r1803582344\",            \"users\": [\"jzxcd\", \"chihangwang\", \"henry60603\"],            \"summary\": \"Discussion on using `**kwargs` for parameters in `OllamaClient`.\",            \"details\": \"User `jzxcd` suggested making `parameters` as `**kwargs` for better scalability. `chihangwang` asked for clarification on the use case for `**kwargs`, preferring to stick to basics for now. `henry60603` provided a link to a discussion on the topic.\",            \"eval_aspect\": [\"code quality\", \"design\"],            \"lead_to_action\": \"code change\",            \"lead_to_action_desc\": \"Consider refactoring to use `**kwargs` for better scalability.\"        },        {            \"parent_thread_id\": 1804937754,            \"child_thread_ids\": [1805790021],            \"html_url\": \"https://github.com/Perfeed/perfeed/pull/8#discussion_r1804937754\",            \"users\": [\"henry60603\", \"chihangwang\"],            \"summary\": \"Discussion on initializing the client with a desired model.\",            \"details\": \"User `henry60603` inquired whether the client could be initialized with a desired model or if it should allow changing the model for each `chat_completion`. `chihangwang` confirmed that the model is now moved to `__init__`.\",            \"eval_aspect\": [\"design\"],            \"lead_to_action\": \"code change\",            \"lead_to_action_desc\": \"Refactor to ensure model initialization is handled correctly.\"        },        {            \"parent_thread_id\": 1805828833,            \"child_thread_ids\": [1805832099, 1805835901, 1805855949],            \"html_url\": \"https://github.com/Perfeed/perfeed/pull/8#discussion_r1805828833\",            \"users\": [\"jzxcd\", \"chihangwang\"],            \"summary\": \"Discussion on passing options in `chat_completion` method.\",            \"details\": \"User `jzxcd` suggested using `options = kwargs`, while `chihangwang` expressed concerns about type checking and the risks of not validating keys in `kwargs`. `chihangwang` emphasized the need for a flexible interface for different LLM providers.\",            \"eval_aspect\": [\"code quality\", \"design\"],            \"lead_to_action\": \"no action\",            \"lead_to_action_desc\": \"Current implementation is acceptable, but future redesign may be needed.\"        }    ]}\n"
     ]
    }
   ],
   "source": [
    "summarizer = PRSummarizer(\n",
    "    GithubProvider(\"Perfeed\"), \n",
    "    llm=OpenAIClient(\"gpt-4o-mini\")\n",
    "    # llm=OllamaClient(\"llama3.1\")\n",
    ")\n",
    "pr_summary, metadata = asyncio.run(summarizer.run(\"perfeed\", 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': ['Enhancement'],\n",
       " 'title': 'Add BaseClient for llm interface with OpenAI and Ollama impl',\n",
       " 'description': 'This PR introduces a new `BaseClient` interface for LLM clients, along with implementations for `OpenAIClient` and `OllamaClient`. This structure allows for more flexible integration of various LLM clients. Additionally, it updates the `pyproject.toml` and `poetry.lock` files to include the new `ollama` package.',\n",
       " 'pr_files': [{'filename': 'poetry.lock',\n",
       "   'language': 'plaintext',\n",
       "   'changes_summary': 'Added `ollama` package and updated dependencies for `openai` and `scikit_learn`.',\n",
       "   'changes_title': 'Update dependencies and add ollama package',\n",
       "   'label': 'dependencies'},\n",
       "  {'filename': 'pyproject.toml',\n",
       "   'language': 'plaintext',\n",
       "   'changes_summary': 'Included `ollama` package in the project dependencies.',\n",
       "   'changes_title': 'Add ollama package to project dependencies',\n",
       "   'label': 'dependencies'},\n",
       "  {'filename': 'src/llms/base_client.py',\n",
       "   'language': 'python',\n",
       "   'changes_summary': 'Created `BaseClient` abstract class defining the interface for LLM clients.',\n",
       "   'changes_title': 'Define BaseClient interface for LLM clients',\n",
       "   'label': 'enhancement'},\n",
       "  {'filename': 'src/llms/ollama_client.py',\n",
       "   'language': 'python',\n",
       "   'changes_summary': 'Implemented `OllamaClient` class that extends `BaseClient` for interacting with the Ollama API.',\n",
       "   'changes_title': 'Implement OllamaClient for Ollama API',\n",
       "   'label': 'enhancement'},\n",
       "  {'filename': 'src/llms/openai_client.py',\n",
       "   'language': 'python',\n",
       "   'changes_summary': 'Implemented `OpenAIClient` class that extends `BaseClient` for interacting with the OpenAI API.',\n",
       "   'changes_title': 'Implement OpenAIClient for OpenAI API',\n",
       "   'label': 'enhancement'}],\n",
       " 'comments': [{'parent_thread_id': 1803582344,\n",
       "   'child_thread_ids': [1803990837, 1804912347],\n",
       "   'users': ['jzxcd', 'chihangwang', 'henry60603'],\n",
       "   'html_url': 'https://github.com/Perfeed/perfeed/pull/8#discussion_r1803582344',\n",
       "   'summary': 'Discussion on using `**kwargs` for parameters in `OllamaClient`.',\n",
       "   'details': 'User `jzxcd` suggested making `parameters` as `**kwargs` for better scalability. `chihangwang` asked for clarification on the use case for `**kwargs`, preferring to stick to basics for now. `henry60603` provided a link to a discussion on the topic.',\n",
       "   'eval_aspect': ['code quality', 'design'],\n",
       "   'lead_to_action': 'code change',\n",
       "   'lead_to_action_desc': 'Consider refactoring to use `**kwargs` for better scalability.'},\n",
       "  {'parent_thread_id': 1804937754,\n",
       "   'child_thread_ids': [1805790021],\n",
       "   'users': ['henry60603', 'chihangwang'],\n",
       "   'html_url': 'https://github.com/Perfeed/perfeed/pull/8#discussion_r1804937754',\n",
       "   'summary': 'Discussion on initializing the client with a desired model.',\n",
       "   'details': 'User `henry60603` inquired whether the client could be initialized with a desired model or if it should allow changing the model for each `chat_completion`. `chihangwang` confirmed that the model is now moved to `__init__`.',\n",
       "   'eval_aspect': ['design'],\n",
       "   'lead_to_action': 'code change',\n",
       "   'lead_to_action_desc': 'Refactor to ensure model initialization is handled correctly.'},\n",
       "  {'parent_thread_id': 1805828833,\n",
       "   'child_thread_ids': [1805832099, 1805835901, 1805855949],\n",
       "   'users': ['jzxcd', 'chihangwang'],\n",
       "   'html_url': 'https://github.com/Perfeed/perfeed/pull/8#discussion_r1805828833',\n",
       "   'summary': 'Discussion on passing options in `chat_completion` method.',\n",
       "   'details': 'User `jzxcd` suggested using `options = kwargs`, while `chihangwang` expressed concerns about type checking and the risks of not validating keys in `kwargs`. `chihangwang` emphasized the need for a flexible interface for different LLM providers.',\n",
       "   'eval_aspect': ['code quality', 'design'],\n",
       "   'lead_to_action': 'no action',\n",
       "   'lead_to_action_desc': 'Current implementation is acceptable, but future redesign may be needed.'}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4o-mini\n",
    "json.loads(pr_summary.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': ['Enhancement'],\n",
       " 'title': 'Add BaseClient for llm interface with OpenAI and Ollama impl',\n",
       " 'description': 'This PR introduces a new `BaseClient` interface for LLM clients, along with implementations for `OpenAIClient` and `OllamaClient`. This structure allows for more flexible integration of various LLM clients. Additionally, it updates the `pyproject.toml` and `poetry.lock` files to include the new `ollama` package.',\n",
       " 'pr_files': [{'filename': 'poetry.lock',\n",
       "   'language': 'plaintext',\n",
       "   'changes_summary': 'Added `ollama` package and updated dependencies for `openai` and `scikit_learn`.',\n",
       "   'changes_title': 'Update dependencies and add ollama package',\n",
       "   'label': 'dependencies'},\n",
       "  {'filename': 'pyproject.toml',\n",
       "   'language': 'plaintext',\n",
       "   'changes_summary': 'Included `ollama` package in the project dependencies.',\n",
       "   'changes_title': 'Add ollama package to project dependencies',\n",
       "   'label': 'dependencies'},\n",
       "  {'filename': 'src/llms/base_client.py',\n",
       "   'language': 'python',\n",
       "   'changes_summary': 'Created `BaseClient` abstract class defining the interface for LLM clients.',\n",
       "   'changes_title': 'Define BaseClient interface for LLM clients',\n",
       "   'label': 'enhancement'},\n",
       "  {'filename': 'src/llms/ollama_client.py',\n",
       "   'language': 'python',\n",
       "   'changes_summary': 'Implemented `OllamaClient` class that extends `BaseClient` for interacting with the Ollama API.',\n",
       "   'changes_title': 'Implement OllamaClient for Ollama API',\n",
       "   'label': 'enhancement'},\n",
       "  {'filename': 'src/llms/openai_client.py',\n",
       "   'language': 'python',\n",
       "   'changes_summary': 'Implemented `OpenAIClient` class that extends `BaseClient` for interacting with the OpenAI API.',\n",
       "   'changes_title': 'Implement OpenAIClient for OpenAI API',\n",
       "   'label': 'enhancement'}],\n",
       " 'comments': [{'parent_thread_id': 1803582344,\n",
       "   'child_thread_ids': [1803990837, 1804912347],\n",
       "   'users': ['jzxcd', 'chihangwang', 'henry60603'],\n",
       "   'html_url': 'https://github.com/Perfeed/perfeed/pull/8#discussion_r1803582344',\n",
       "   'summary': 'Discussion on using `**kwargs` for parameters in `OllamaClient`.',\n",
       "   'details': 'User `jzxcd` suggested making `parameters` as `**kwargs` for better scalability. `chihangwang` asked for clarification on the use case for `**kwargs`, preferring to stick to basics for now. `henry60603` provided a link to a discussion on the topic.',\n",
       "   'eval_aspect': ['code quality', 'design'],\n",
       "   'lead_to_action': 'code change',\n",
       "   'lead_to_action_desc': 'Consider refactoring to use `**kwargs` for better scalability.'},\n",
       "  {'parent_thread_id': 1804937754,\n",
       "   'child_thread_ids': [1805790021],\n",
       "   'users': ['henry60603', 'chihangwang'],\n",
       "   'html_url': 'https://github.com/Perfeed/perfeed/pull/8#discussion_r1804937754',\n",
       "   'summary': 'Discussion on initializing the client with a desired model.',\n",
       "   'details': 'User `henry60603` inquired whether the client could be initialized with a desired model or if it should allow changing the model for each `chat_completion`. `chihangwang` confirmed that the model is now moved to `__init__`.',\n",
       "   'eval_aspect': ['design'],\n",
       "   'lead_to_action': 'code change',\n",
       "   'lead_to_action_desc': 'Refactor to ensure model initialization is handled correctly.'},\n",
       "  {'parent_thread_id': 1805828833,\n",
       "   'child_thread_ids': [1805832099, 1805835901, 1805855949],\n",
       "   'users': ['jzxcd', 'chihangwang'],\n",
       "   'html_url': 'https://github.com/Perfeed/perfeed/pull/8#discussion_r1805828833',\n",
       "   'summary': 'Discussion on passing options in `chat_completion` method.',\n",
       "   'details': 'User `jzxcd` suggested using `options = kwargs`, while `chihangwang` expressed concerns about type checking and the risks of not validating keys in `kwargs`. `chihangwang` emphasized the need for a flexible interface for different LLM providers.',\n",
       "   'eval_aspect': ['code quality', 'design'],\n",
       "   'lead_to_action': 'no action',\n",
       "   'lead_to_action_desc': 'Current implementation is acceptable, but future redesign may be needed.'}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llama 3.1\n",
    "json.loads(pr_summary.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = PRSummarizer(\n",
    "    GithubProvider(\"Perfeed\"), \n",
    "    llm=OllamaClient(\"llama3.1\")\n",
    ")\n",
    "fs = FeatherStorage(data_type=\"pr_summary\", overwrite=False, append=True)\n",
    "# for pr in [11, 12, 13, 14]:\n",
    "#     pr_summary, metadata = asyncio.run(summarizer.run(\"perfeed\", pr))\n",
    "#     fs.save(data=pr_summary, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fs.load()\n",
    "rank = df.groupby(['pr_number', 'llm_provider', 'model'])['created_at'].rank(ascending=False)\n",
    "df = df[rank==1]\n",
    "df['review_interval'] = (pd.to_datetime(df['pr_merged_at']) - pd.to_datetime(df['pr_created_at'])).dt.total_seconds() / 3600\n",
    "df = df[df['author'] == 'jzxcd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, StrictUndefined\n",
    "from perfeed.config_loader import settings\n",
    "from perfeed.utils.utils import json_output_curator\n",
    "\n",
    "llm_llama = OllamaClient(\"llama3.1\")\n",
    "llm_openai = OpenAIClient(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variables = {\n",
    "    \"pr_summaries\": df.to_json(orient='records'),\n",
    "}\n",
    "environment = Environment(undefined=StrictUndefined)\n",
    "system_prompt = environment.from_string(\n",
    "    settings.weekly_summary_prompt.system\n",
    ").render(variables)\n",
    "user_prompt = environment.from_string(\n",
    "    settings.weekly_summary_prompt.user\n",
    ").render(variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**PR Summary for the Week**\n",
       "\n",
       "### Overview of PRs\n",
       "\n",
       "This week, there were two pull requests (PRs) merged into the Perfeed repository.\n",
       "\n",
       "#### PR 1: Fixing pr_summarizer for Proper Format and Asyncio Run\n",
       "Type: Bug fix\n",
       "Title: fix pr_summarizer for proper format and asyncio run\n",
       "Description: This PR fixes the `pr_summarizer` to properly output JSON format and adds asyncio support. It also restructures the utils.\n",
       "\n",
       "#### PR 2: Adding Support for Feather and SQL DB Storage\n",
       "Type: Bug fix\n",
       "Title: Datastore\n",
       "Description: Added support for feather and SQL DB storage. Also added save and load functionality.\n",
       "\n",
       "### Significant Changes\n",
       "\n",
       "**PR 1**\n",
       "\n",
       "* **Bug Fix**: Updated `pr_summarizer` to properly output JSON format and added asyncio support.\n",
       "* **Enhancement**: Restructured the utils module, added _output_curator function to the utils module, and renamed tokenizer.py to utils.py.\n",
       "\n",
       "**PR 2**\n",
       "\n",
       "* **Bug Fix**: Added support for feather and SQL DB storage, and added save and load functionality.\n",
       "* **Discussion Points**:\n",
       "\t+ Discussion on async function and PRSummarizer.run()\n",
       "\t+ Discussion on PRSummaryMetadata class\n",
       "\t+ Discussion on OllamaClient class\n",
       "\t+ Discussion on DataStore class and factory function\n",
       "\t+ Discussion on BaseStorage class and return type\n",
       "\t+ Discussion on validate_and_convert function\n",
       "\n",
       "### Review Interval\n",
       "\n",
       "The review interval for both PRs was within a reasonable timeframe.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "Two important PRs were merged into the Perfeed repository this week, fixing issues with `pr_summarizer` and adding support for feather and SQL DB storage. The discussions on these PRs provided valuable insights and suggestions from team members."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llama3.1\n",
    "summary = llm_llama.chat_completion(system_prompt, user_prompt)\n",
    "Markdown(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Summary of Recent PRs\n",
       "\n",
       "This week, the team merged 2 PRs focused on bug fixes and enhancements to the `pr_summarizer` and datastore functionalities.\n",
       "\n",
       "---\n",
       "\n",
       "**Overview:**\n",
       "- The first PR addressed issues with the `pr_summarizer`, ensuring proper JSON output and adding asyncio support. The second PR introduced support for feather and SQL database storage, along with save and load functionalities.\n",
       "\n",
       "---\n",
       "\n",
       "**Significant Changes:**\n",
       "- **PR Summarizer Fixes:**\n",
       "  - Fixed JSON output formatting and added asyncio support to enhance performance.\n",
       "  - Restructured utility functions for better organization.\n",
       "  \n",
       "- **Datastore Enhancements:**\n",
       "  - Introduced a base class for storage handlers, allowing for easier management of different storage types.\n",
       "  - Added classes for feather and SQL database storage, enabling flexible data handling.\n",
       "\n",
       "---\n",
       "\n",
       "**Refactors/Architecture:**\n",
       "- Renamed `tokenizer.py` to `utils.py` to better reflect its purpose.\n",
       "- Updated the `.gitignore` file to exclude unnecessary data files.\n",
       "- Created an `__init__.py` file for the data stores to facilitate package management.\n",
       "\n",
       "---\n",
       "\n",
       "**Review Process:**\n",
       "- For the `pr_summarizer` PR, discussions included the use of the async keyword and the purpose of the `PRSummaryMetadata` class, with clarifications provided by the author, jzxcd.\n",
       "- In the datastore PR, user chihangwang raised several questions regarding the design choices, such as the factory function and return types. jzxcd engaged in discussions, agreeing to some suggestions while justifying others based on usability.\n",
       "- Both PRs were merged efficiently, with the first PR taking approximately 4 hours and the second around 40 hours to address feedback and finalize changes.\n",
       "\n",
       "--- \n",
       "\n",
       "This summary aligns with our sprint objectives by enhancing the functionality and performance of our tools, ensuring a more robust and maintainable codebase."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt-4o-mini\n",
    "summary = llm_openai.chat_completion(system_prompt, user_prompt)\n",
    "Markdown(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
