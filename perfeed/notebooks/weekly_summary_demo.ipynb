{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perfeed.tools.pr_summarizer import PRSummarizer\n",
    "from perfeed.git_providers.github import GithubProvider\n",
    "from perfeed.llms.ollama_client import OllamaClient\n",
    "from perfeed.llms.openai_client import OpenAIClient\n",
    "from perfeed.data_stores import FeatherStorage\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PR summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{    \"type\": [\"Enhancement\"],    \"title\": \"Add BaseClient for llm interface with OpenAI and Ollama impl\",    \"description\": \"This PR introduces a new `BaseClient` interface for LLM clients, along with implementations for `OpenAIClient` and `OllamaClient`. The `OllamaClient` allows for chat completions using the Ollama API, while the `OpenAIClient` integrates with OpenAI's API, ensuring flexibility for various LLM clients. Additionally, it updates the `poetry.lock` and `pyproject.toml` files to include the necessary dependencies.\",    \"pr_files\": [        {            \"filename\": \"poetry.lock\",            \"language\": \"TOML\",            \"changes_summary\": \"Added dependencies for `ollama` and updated existing dependencies.\",            \"changes_title\": \"Update dependencies for LLM clients\",            \"label\": \"dependencies\"        },        {            \"filename\": \"pyproject.toml\",            \"language\": \"TOML\",            \"changes_summary\": \"Included `ollama` as a dependency for the project.\",            \"changes_title\": \"Add ollama dependency\",            \"label\": \"dependencies\"        },        {            \"filename\": \"src/llms/base_client.py\",            \"language\": \"Python\",            \"changes_summary\": \"Created a new abstract class `BaseClient` to define the interface for LLM clients.\",            \"changes_title\": \"Define BaseClient interface\",            \"label\": \"enhancement\"        },        {            \"filename\": \"src/llms/ollama_client.py\",            \"language\": \"Python\",            \"changes_summary\": \"Implemented `OllamaClient` that extends `BaseClient` for chat completions using the Ollama API.\",            \"changes_title\": \"Implement OllamaClient\",            \"label\": \"enhancement\"        },        {            \"filename\": \"src/llms/openai_client.py\",            \"language\": \"Python\",            \"changes_summary\": \"Implemented `OpenAIClient` that extends `BaseClient` for chat completions using the OpenAI API.\",            \"changes_title\": \"Implement OpenAIClient\",            \"label\": \"enhancement\"        }    ],    \"comments\": [        {            \"parent_thread_id\": 1803582344,            \"child_thread_ids\": [1803990837, 1804912347],            \"html_url\": \"https://github.com/Perfeed/perfeed/pull/8#discussion_r1803582344\",            \"users\": [\"jzxcd\", \"chihangwang\", \"henry60603\"],            \"summary\": \"Discussion on using `**kwargs` for better scalability in `OllamaClient`.\",            \"eval_aspect\": [\"code change\", \"clarification\"],            \"lead_to_action\": \"code change\",            \"lead_to_action_desc\": \"Considered the suggestion to use `**kwargs` but opted to keep the current implementation for simplicity.\"        },        {            \"parent_thread_id\": 1804937754,            \"child_thread_ids\": [1805790021],            \"html_url\": \"https://github.com/Perfeed/perfeed/pull/8#discussion_r1804937754\",            \"users\": [\"henry60603\", \"chihangwang\"],            \"summary\": \"Question about initializing the client with a desired model.\",            \"eval_aspect\": [\"code change\", \"clarification\"],            \"lead_to_action\": \"code change\",            \"lead_to_action_desc\": \"Moved model initialization to the constructor for better design.\"        },        {            \"parent_thread_id\": 1805828833,            \"child_thread_ids\": [1805832099, 1805835901, 1805855949],            \"html_url\": \"https://github.com/Perfeed/perfeed/pull/8#discussion_r1805828833\",            \"users\": [\"jzxcd\", \"chihangwang\"],            \"summary\": \"Discussion on passing options to the `chat_completion` method.\",            \"eval_aspect\": [\"code change\", \"clarification\"],            \"lead_to_action\": \"no action\",            \"lead_to_action_desc\": \"Decided to keep the current method of passing options for flexibility.\"        }    ]}\n"
     ]
    }
   ],
   "source": [
    "summarizer = PRSummarizer(\n",
    "    GithubProvider(\"Perfeed\"), \n",
    "    llm=OpenAIClient(\"gpt-4o-mini\")\n",
    "    # llm=OllamaClient(\"llama3.1\")\n",
    ")\n",
    "pr_summary, metadata = asyncio.run(summarizer.run(\"perfeed\", 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': ['Enhancement'],\n",
       " 'title': 'Add BaseClient for llm interface with OpenAI and Ollama impl',\n",
       " 'description': \"This PR introduces a new `BaseClient` interface for LLM clients, along with implementations for `OpenAIClient` and `OllamaClient`. The `OllamaClient` allows for chat completions using the Ollama API, while the `OpenAIClient` integrates with OpenAI's API, ensuring flexibility for various LLM clients. Additionally, it updates the `poetry.lock` and `pyproject.toml` files to include the necessary dependencies.\",\n",
       " 'pr_files': [{'filename': 'poetry.lock',\n",
       "   'language': 'TOML',\n",
       "   'changes_summary': 'Added dependencies for `ollama` and updated existing dependencies.',\n",
       "   'changes_title': 'Update dependencies for LLM clients',\n",
       "   'label': 'dependencies'},\n",
       "  {'filename': 'pyproject.toml',\n",
       "   'language': 'TOML',\n",
       "   'changes_summary': 'Included `ollama` as a dependency for the project.',\n",
       "   'changes_title': 'Add ollama dependency',\n",
       "   'label': 'dependencies'},\n",
       "  {'filename': 'src/llms/base_client.py',\n",
       "   'language': 'Python',\n",
       "   'changes_summary': 'Created a new abstract class `BaseClient` to define the interface for LLM clients.',\n",
       "   'changes_title': 'Define BaseClient interface',\n",
       "   'label': 'enhancement'},\n",
       "  {'filename': 'src/llms/ollama_client.py',\n",
       "   'language': 'Python',\n",
       "   'changes_summary': 'Implemented `OllamaClient` that extends `BaseClient` for chat completions using the Ollama API.',\n",
       "   'changes_title': 'Implement OllamaClient',\n",
       "   'label': 'enhancement'},\n",
       "  {'filename': 'src/llms/openai_client.py',\n",
       "   'language': 'Python',\n",
       "   'changes_summary': 'Implemented `OpenAIClient` that extends `BaseClient` for chat completions using the OpenAI API.',\n",
       "   'changes_title': 'Implement OpenAIClient',\n",
       "   'label': 'enhancement'}],\n",
       " 'comments': [{'parent_thread_id': 1803582344,\n",
       "   'child_thread_ids': [1803990837, 1804912347],\n",
       "   'users': ['jzxcd', 'chihangwang', 'henry60603'],\n",
       "   'html_url': 'https://github.com/Perfeed/perfeed/pull/8#discussion_r1803582344',\n",
       "   'summary': 'Discussion on using `**kwargs` for better scalability in `OllamaClient`.',\n",
       "   'eval_aspect': ['code change', 'clarification'],\n",
       "   'lead_to_action': 'code change',\n",
       "   'lead_to_action_desc': 'Considered the suggestion to use `**kwargs` but opted to keep the current implementation for simplicity.'},\n",
       "  {'parent_thread_id': 1804937754,\n",
       "   'child_thread_ids': [1805790021],\n",
       "   'users': ['henry60603', 'chihangwang'],\n",
       "   'html_url': 'https://github.com/Perfeed/perfeed/pull/8#discussion_r1804937754',\n",
       "   'summary': 'Question about initializing the client with a desired model.',\n",
       "   'eval_aspect': ['code change', 'clarification'],\n",
       "   'lead_to_action': 'code change',\n",
       "   'lead_to_action_desc': 'Moved model initialization to the constructor for better design.'},\n",
       "  {'parent_thread_id': 1805828833,\n",
       "   'child_thread_ids': [1805832099, 1805835901, 1805855949],\n",
       "   'users': ['jzxcd', 'chihangwang'],\n",
       "   'html_url': 'https://github.com/Perfeed/perfeed/pull/8#discussion_r1805828833',\n",
       "   'summary': 'Discussion on passing options to the `chat_completion` method.',\n",
       "   'eval_aspect': ['code change', 'clarification'],\n",
       "   'lead_to_action': 'no action',\n",
       "   'lead_to_action_desc': 'Decided to keep the current method of passing options for flexibility.'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4o-mini\n",
    "json.loads(pr_summary.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': ['Enhancement'],\n",
       " 'title': 'Add BaseClient for llm interface with OpenAI and Ollama impl',\n",
       " 'description': 'This pull request adds a base client class for interacting with various language models (LLMs) like OpenAI and Ollama. It includes two new classes, `OpenAIClient` and `OllamaClient`, which inherit from the `BaseClient` class. These clients provide an interface to interact with their respective LLMs.',\n",
       " 'pr_files': [{'filename': 'src/llms/base_client.py',\n",
       "   'language': 'Python',\n",
       "   'changes_summary': 'Added a base client class for interacting with various language models (LLMs). Two new classes, `OpenAIClient` and `OllamaClient`, were added to inherit from the `BaseClient` class.',\n",
       "   'changes_title': 'Add Base Client Class',\n",
       "   'label': 'Enhancement'},\n",
       "  {'filename': 'src/llms/openai_client.py',\n",
       "   'language': 'Python',\n",
       "   'changes_summary': 'Added an OpenAI client class that inherits from the `BaseClient` class. It provides an interface to interact with the OpenAI API.',\n",
       "   'changes_title': 'Add OpenAI Client Class',\n",
       "   'label': 'Enhancement'},\n",
       "  {'filename': 'src/llms/ollama_client.py',\n",
       "   'language': 'Python',\n",
       "   'changes_summary': 'Added an Ollama client class that inherits from the `BaseClient` class. It provides an interface to interact with the Ollama API.',\n",
       "   'changes_title': 'Add Ollama Client Class',\n",
       "   'label': 'Enhancement'}],\n",
       " 'comments': [{'parent_thread_id': 1803582344,\n",
       "   'child_thread_ids': [1803990837, 1804912347],\n",
       "   'users': ['jzxcd', 'chihangwang'],\n",
       "   'html_url': 'https://github.com/Perfeed/perfeed/pull/8#discussion_r1803582344',\n",
       "   'summary': 'Discussion about passing `parameters` as `**kwargs` for better scalability and creating another method to set default values.',\n",
       "   'eval_aspect': ['Code Change'],\n",
       "   'lead_to_action': 'No Action',\n",
       "   'lead_to_action_desc': ''},\n",
       "  {'parent_thread_id': 1804937754,\n",
       "   'child_thread_ids': [1805790021],\n",
       "   'users': ['henry60603', 'chihangwang'],\n",
       "   'html_url': 'https://github.com/Perfeed/perfeed/pull/8#discussion_r1804937754',\n",
       "   'summary': 'Discussion about initializing the client with a desired model or changing the model for each `chat_completion`.',\n",
       "   'eval_aspect': ['Code Change'],\n",
       "   'lead_to_action': 'No Action',\n",
       "   'lead_to_action_desc': ''},\n",
       "  {'parent_thread_id': 1805828833,\n",
       "   'child_thread_ids': [1805832099, 1805835901, 1805855949],\n",
       "   'users': ['jzxcd', 'chihangwang'],\n",
       "   'html_url': 'https://github.com/Perfeed/perfeed/pull/8#discussion_r1805828833',\n",
       "   'summary': 'Discussion about redesigning the options-passing interface for better flexibility.',\n",
       "   'eval_aspect': ['Code Change'],\n",
       "   'lead_to_action': 'No Action',\n",
       "   'lead_to_action_desc': ''}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llama 3.1\n",
    "json.loads(pr_summary.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = PRSummarizer(\n",
    "    GithubProvider(\"Perfeed\"), \n",
    "    llm=OllamaClient(\"llama3.1\")\n",
    ")\n",
    "fs = FeatherStorage(data_type=\"pr_summary\", overwrite=False, append=True)\n",
    "# for pr in [11, 12, 13, 14]:\n",
    "#     pr_summary, metadata = asyncio.run(summarizer.run(\"perfeed\", pr))\n",
    "#     fs.save(data=pr_summary, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fs.load()\n",
    "rank = df.groupby(['pr_number', 'llm_provider', 'model'])['created_at'].rank(ascending=False)\n",
    "df = df[rank==1]\n",
    "df['review_interval'] = (pd.to_datetime(df['pr_merged_at']) - pd.to_datetime(df['pr_created_at'])).dt.total_seconds() / 86400\n",
    "df = df[df['author'] == 'jzxcd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, StrictUndefined\n",
    "from perfeed.config_loader import settings\n",
    "from perfeed.utils.utils import json_output_curator\n",
    "\n",
    "llm_llama = OllamaClient(\"llama3.1\")\n",
    "llm_openai = OpenAIClient(\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variables = {\n",
    "    \"pr_summaries\": df.to_json(orient='records'),\n",
    "}\n",
    "environment = Environment(undefined=StrictUndefined)\n",
    "system_prompt = environment.from_string(\n",
    "    settings.weekly_summary_prompt.system\n",
    ").render(variables)\n",
    "user_prompt = environment.from_string(\n",
    "    settings.weekly_summary_prompt.user\n",
    ").render(variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**PR Summary for the Week**\n",
       "\n",
       "This week, jzxcd merged 2 PRs that addressed critical issues in Perfeed.\n",
       "\n",
       "### Overview\n",
       "The two PRs focused on bug fixes and enhancements to improve performance and functionality.\n",
       "\n",
       "* **PR 13:** This PR fixed inconsistencies in the `pr_summarizer` output and added asyncio support. It also restructured the `utils` module.\n",
       "* **PR 14:** This PR added support for feather and SQL DB storage, along with save and load functionality.\n",
       "\n",
       "### Significant Changes\n",
       "#### PR 13:\n",
       "* Renamed `tokenizer.py` to `utils.py` and refactored the `utils` module.\n",
       "* Added asyncio support to the `pr_summarizer`.\n",
       "* Refactored the `utils` module to improve maintainability.\n",
       "\n",
       "#### PR 14:\n",
       "* Added support for feather and SQL DB storage.\n",
       "* Introduced save and load functionality.\n",
       "* Moved the `validate_and_convert` function to the `BaseStorage` class.\n",
       "\n",
       "### Noteworthy Refactors/Architecture\n",
       "None\n",
       "\n",
       "### Review Process\n",
       "Both PRs had thorough discussions on GitHub, with multiple code changes and suggestions from contributors. The issues were addressed, and the PRs were merged successfully.\n",
       "\n",
       "**PR 13:**\n",
       "\n",
       "* Created on: 2024-10-26T23:48:11Z\n",
       "* Merged on: 2024-10-28T16:13:24Z\n",
       "* Review interval: 1.6841782407\n",
       "\n",
       "**PR 14:**\n",
       "\n",
       "* Created on: 2024-10-26T23:48:11Z\n",
       "* Merged on: 2024-10-28T16:13:24Z\n",
       "* Review interval: 1.6841782407"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llama3.1\n",
    "summary = llm_llama.chat_completion(system_prompt, user_prompt)\n",
    "Markdown(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### PR Summary for the Week\n",
       "\n",
       "This week, the team focused on bug fixes and enhancements, merging two significant pull requests.\n",
       "\n",
       "---\n",
       "\n",
       "#### Overview:\n",
       "- The first PR addressed issues with the `pr_summarizer`, ensuring proper JSON output and adding asyncio support. The second PR introduced support for feather and SQL database storage, along with save and load functionalities.\n",
       "\n",
       "---\n",
       "\n",
       "#### Significant Changes:\n",
       "1. **PR Summarizer Fixes and Improvements**:\n",
       "   - Fixed JSON output formatting in the `pr_summarizer`.\n",
       "   - Added asyncio support to enhance performance.\n",
       "   - Refactored the `utils` module to improve structure and maintainability.\n",
       "\n",
       "2. **Datastore Enhancements**:\n",
       "   - Introduced `BaseStorage` class for handling different storage types.\n",
       "   - Added `FeatherStorage` and `SQLStorage` classes for feather and SQL database support, respectively.\n",
       "   - Updated `.gitignore` to exclude unnecessary files.\n",
       "\n",
       "---\n",
       "\n",
       "#### Refactors/Architecture:\n",
       "- The `utils` module was restructured, including renaming `tokenizer.py` to `utils.py` and adding the `_output_curator` function.\n",
       "- The `DataStore` class factory function was removed to simplify the architecture.\n",
       "- The `validate_and_convert` function was moved to the `BaseStorage` class, promoting better organization of storage-related logic.\n",
       "\n",
       "---\n",
       "\n",
       "#### Review Process:\n",
       "- For the `pr_summarizer` PR, there were discussions around maintaining async functionality in the `PRSummarizer.run` method, which led to the addition of async support.\n",
       "- In the datastore PR, feedback included the addition of the `PRSummaryMetadata` class and discussions on the `OllamaClient` class enhancements.\n",
       "- The author addressed all comments effectively, leading to both PRs being merged within 4 hours and 1.5 days, respectively, after submission.\n",
       "\n",
       "--- \n",
       "\n",
       "This summary highlights the team's commitment to improving code quality and functionality, aligning with sprint objectives focused on bug fixes and performance enhancements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gpt-4o-mini\n",
    "summary = llm_openai.chat_completion(system_prompt, user_prompt)\n",
    "Markdown(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### PR Summary for the Week\\n\\nThis week, the team focused on bug fixes and enhancements, merging two significant pull requests.\\n\\n---\\n\\n#### Overview:\\n- The first PR addressed issues with the `pr_summarizer`, ensuring proper JSON output and adding asyncio support. The second PR introduced support for feather and SQL database storage, along with save and load functionalities.\\n\\n---\\n\\n#### Significant Changes:\\n1. **PR Summarizer Fixes and Improvements**:\\n   - Fixed JSON output formatting in the `pr_summarizer`.\\n   - Added asyncio support to enhance performance.\\n   - Refactored the `utils` module to improve structure and maintainability.\\n\\n2. **Datastore Enhancements**:\\n   - Introduced `BaseStorage` class for handling different storage types.\\n   - Added `FeatherStorage` and `SQLStorage` classes for feather and SQL database support, respectively.\\n   - Updated `.gitignore` to exclude unnecessary files.\\n\\n---\\n\\n#### Refactors/Architecture:\\n- The `utils` module was restructured, including renaming `tokenizer.py` to `utils.py` and adding the `_output_curator` function.\\n- The `DataStore` class factory function was removed to simplify the architecture.\\n- The `validate_and_convert` function was moved to the `BaseStorage` class, promoting better organization of storage-related logic.\\n\\n---\\n\\n#### Review Process:\\n- For the `pr_summarizer` PR, there were discussions around maintaining async functionality in the `PRSummarizer.run` method, which led to the addition of async support.\\n- In the datastore PR, feedback included the addition of the `PRSummaryMetadata` class and discussions on the `OllamaClient` class enhancements.\\n- The author addressed all comments effectively, leading to both PRs being merged within 4 hours and 1.5 days, respectively, after submission.\\n\\n--- \\n\\nThis summary highlights the team's commitment to improving code quality and functionality, aligning with sprint objectives focused on bug fixes and performance enhancements.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
